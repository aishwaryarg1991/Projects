package com.sundogsoftware.spark

import org.apache.spark.SparkContext
import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._


object AshMinTemperatures {
  
  def main(args: Array[String]){
    
    Logger.getLogger("org").setLevel(Level.ERROR)
    val sc= new SparkContext("local[*]", "AshMinTemperatures") //
    val lines = sc.textFile("c:/SparkScala/temperature.csv")
    val parseLines= lines.map(parseLine)
   // val minTemp= parseLines.filter(x => x._2== "TMIN")
     val minTemps = parsedLines.filter(x => x._2 == "TMIN")
        val results = minTemp.collect()
 
    results.sorted.foreach(println)
  }
  
  def parseLine(line:String)=  {
    val fields = line.split(',')
    val stationID = fields(0)
    val entryType = fields(2)
    val temperature = fields(3).toFloat * 0.1f * (9.0f / 5.0f) + 32.0f
    (stationID, entryType, temperature)    
  }
  
  
}